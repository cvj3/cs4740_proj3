10/30 & 10/31:
	added lines:
		word = word.lower()
		if tag in string.punctuation: tag = ".""
	to predict conditional probability in predict.py...  I thought we were properly lowercasing words and processing punct in POS tags.
	I wanted to add an extra check/layer of enforcement, just in case it wasn't happening.  
	Unit tests (3000) prior to change:
		HMM: 88.6 - 42.4
		Baseline: 90.3 - 88.7
	Post change:
		HMM: 90.3 - 91.8
		Baseline: 90.3 - 88.7
	On Kaggle:
	No IB: 62.792 - identical to last HMM submission.  Must have been an error with the unit tests that we fixed :(


	Using Pr(word | ent) instead of Pr(ent | word) (same for tagged)
	On Kaggle: .60308


	Using a trivial add-one smoothing for unknown words (if word isn't found, we act as if there is only one occurence of it)
	Unit tests (3000): (don't expect much from the unit tests, since all of these words HAVE been seen.)
		HMM: 68.6 - 82.19
		Previously: 90.3 - 91.8
	On Kaggle: .68176!!!!!!! Huge improvement.  We're nearing the baseline.


	Trying out Trigram
		HMM: 60.86 - 66.33
		Previously: 68.6 - 82.19
	On Kaggle: .53528.... ouch


	68-67 on 100 went to 77 - 83 when REMOVING END SCALING (in the termination step)
	Trying on 3000 unit tests:
		HMM-Tri: 66.3 - 75.93
		Previous-Tri: 60.86 - 66.33
		Vs Bigram: 68.6 - 82.19

		HMM-Bi: 68.6 - 82.49 (slight improvement)
		Previous-Bi: 68.6 - 82.19

	Trigram did much better with end scaling removed.  Will run this model on kaggle when submission limit resets
	(btw, its now 10/31, happy halloween, reader!)

	Trying with I- and B- tags added back in.  Maybe trigrams do better with this info.
	77-83 on 100 unit tests went to 68-77  (22 seconds runtime to 86 seconds)

	Without add-one smoothing (to get a better comparison to the baseline) trigram does 90.30 - 61.83 (100 cases) vs 77-83
	Add-one smoothing is bad for unit tests.  It gives value to combinations of entities and words that never happen in unit test cases
	Anyway, 90-61 tells us that our prediction rate is pretty good, but that we're not making enough predictions.


	Optimizing a bit (full csv file production takes 723 seconds = 12 minutes)
	noticed that the sum(____.itervalues) was taking a huge time for the words dictionary, because there are SO many key-value pairs.
	These are static values.  It makes sense to pre-process these, instead of processing it for each iteration.  
	Took a 100 unit test from 22 seconds down to 6 (including pre-processing time)
	Full CSV output (3000 individual sets of test sequences)
	723s (12 min) -> 208s (3.5 min)

	Repeating this for tag sums (this is actually the same value.  using it for both)
	Full CSV output (3000 individual sets of test sequences)
	723s (12 min) -> 205s (3.4 min)

	Moved checking if tag is punctuation out of the probability computations and into the start of each test.
	723s (12 min) -> 200s (3.3 min)

	Missed one words counting (in the add-one naive smoothing) - replaced it with the new dict lookup
	723s (12 min) -> 18s (.3 min)

	Oh.  Unit tests (3000 cases) confirm 66-75.9 (current known best)... we didnt break anything.  Glad I optimized... its now 4000% faster.


	Fixed a bug i found in the trigram (3000 cases)
		HMM-Tri: 67.8 - 81.4
		Previous-BestTri: 66.3 - 75.93

	End scaling added back in (3000 cases)
		did worse.  (62-71). leaving it out

	Changed add 1 smoothing to add 1 in ALL cases (so an unseen isn't equally valued as something that did have one occurence)
		HMM-Tri: 73.7 - 85.6 (very promising!)
		Previous-BestTri: 67.8 - 81.4  (take with grain of salt, definitely helps not fail unit tests as much. may not be as helpful normally)

	No front-scaling? - definitely hurt
		HMM-Tri: 68.6 - 84.4
		Previous-BestTri: 73.7 - 85.6 

	Current state of things: we should see if add one when there is no match is better than always adding one (on kaggle) Will submit:
	trigram-add-one-nomatch-no-end-scaling
	trigram-add-one-always-no-end-scaling

	NOTE: "end state scaling" is where in the termination step, you scale the viterbi score of each state for the last entry by it's likelihood to appear
		at the end of the sequence.  I believe this is hurting because our sequences are not natural endings, our sequences are just truncated.  Thus, its
		ending frequently on the most common entity - "O".  This is causing us to over-predict / over-value an "O" at the end of our sequence.

	Cool note: obviously both models should do super well on training.  But our HMM does much worse than the baseline on training.
	Well, this is because of the smoothing.  There's no reason to give value to things that we know can't have value when testing on our own training data
	Without the smoothing, the trigram HMM does 91.2 - 92.4 compared to the 90.3 - 88.7.  We are beating our baseline locally!!! 
	Hopefully, with smoothing, this will beat the baseline when looking at external test data.

	IMPORTANT TO NOTE: Since we didnt make a reserved set, it's crucial to be aware of things like this.  Without thinking about the influence of testing
	on the same set we trained on, it would seem that the smoothing is hurting our performance, when it's actually helping.  This is probably why you
	shouldn't do this, but if you're actively thinking about what the methodologies you're using will do in this scenario, you can mitigate the adverse
	effects of testing on your training set.

	Bigram vs Trigram (with fixes and optimizations added to bigram):
		Bigram: 73.8 - 86.7
		Trigram: 73.7 - 85.6
	Maybe the bigram is still better :( Will have to submit one of these as well to see

	Current to-submit list:
	trigram-add-one-nomatch-no-end-scaling
	trigram-add-one-always-no-end-scaling
	bigram-add-one-always-no-end-scaling

	Checking IB included again to see if we've been dropping any value not having it.  3000 unit tests: 59-81. worse.

11/1
	trigram-add-one-nomatch-no-end-scaling - 69.2
	trigram-add-one-always-no-end-scaling - 71.2
	bigram-add-one-always-no-end-scaling - 70.4

	trigram with always add one is our current best (non-baseline)

	trigram with stemming of words is at 71.619 on kaggle (NEW BEST)
		HMM-Tri-Stemming: 74.8 - 85.2
		Bigram-Tri-Stemmin: 74.6 - 86.1

	Added tracking how many errors happens for each entity during unit testing.  MISC was very high.
	Tried down-scaling misc probability predictions: 	if entity.lower().strip() == "misc": one *= .5
	Locally, 79-85, but on kaggle: 71.427

	Next - consider a different smoothing.  For unseen words, maybe add them to the dictionary in memory once seen?


	88.2 - 77.6 - Potential new best? (on unit training)
	Now scaling by count(ent) / total number of ents seen - scaling by likelihood of seeing the entity.
	We get much better accuracy in terms of predictions made, but make less predictions.  
	New Kaggle best!!! 73.167!

	looked into adding some simple rules... but these technically should be evident from the probabilities during training... none of the rules helped much.

	Our new best now has a lower correct/expected count, which means we are now making more "O" predictions than desireable.
	One simple approach = if ent is O, then reduce the scaling factor by .50.
	This brought us to 86.3 - 81.76, which may do better than 88.2 - 77.6.  Will have to wait till 8 to see!
	Current Best: 88.2 - 77.6 - (Total: 165.8)
	Manual Scalings of O Likelihood:
		0.5: 86.3 - 81.76 (Total: 168.1)
		.25: 84.5 - 83.8  (Total: 168.3)
		.35: 85.7 - 82.5  (Total: 168.3)
		.75: 87.6 - 79.3  (Total: 166.9)

	So, our trigram does well on the training data without add-one smoothing.  What if we added one more intelligently, so that we maintain this level of 
	performance for test data that looks similar to our training data (like 90% accuracy), and only allow an accuracy drop when the word is truly unknown.
	Currently our smoothing smooths pr(word | ent) so it's adding one to a word that we have seen for ents we've never seen it with.  What if we only did this
	when we haven't seen the word at all?

		now on unit test: 93.1 - 88.1 (with .5 downscaling on O likelihood) Kaggle: 77.249
						  93.5 - 86.0 (without downscaling)	KAGGLE: 78.302 - NEW BEST WOOOOOOOOOOOO

	93.46 - 86.02 is our new best unit score.  
	90.7 - 91.49 is our score without scaling by ent likelihood. - Kaggle: 78.147 (slight decrease)

11/7

	So, let's say we do smooth an unknown.  Compare this to something we've seen once.
	Should these have the same value?  What if we added one to the count of every seen
	value (only for the word - ent and pos - ent pairs we've SEEN, not all possible
	pairs like we do for the unknown words.)  Would this help value seen examples
	more heavily?

	Also, (even though it was a slight hit on kaggle) - going to remove the ent likelihood scaling.
	it's not what viterbi says to do and the likelihoods are already supposed to be built in.  It's likely
	just a random variation and we shouldn't base our approach on it.

	Locally, this shouldn't make much of a difference:
	90.67 - 91.78 locally
	"trigram-add-one-to-seen" - Kaggle: 77.840 - worse

	looking into using a supplemental lookup table for possible unknown words.
	if a word isn't in our training, but DOES appear in the supplemental source, we only assign points for the entity
	it is labeled with in the supplement.

	Locally - no change
	On kaggle: HUGE improvement - 84.566!
	On kaggle with ent scaling - 84.789 with ent scaling.  It seems to be better, but i'm still not sure it's a strict
	improvement.  It may just happen to be an improvement for the two entities it's testing on.  I hesitate to use this
	for the final submissions

	Added logic: if we use the supplement to look up a word, ignore the POS likelihood if the likelihood is 0 (dont let
	POS pr = 0 erase our supplement lookup.)
	This only improved Kaggle score by 0.02%, but it's something!

	Maybe we aren't using as much of the supplemental list as we could be.  Currently, ignores any phrases that have multiple words (doesnt add it to supplement list)
	Trivial solution: first word is always taken and added.  May want to look into an even better / more complex solution
	later.  This means that if the same word is seen mutliple times, it just overwrites the previous association for it.
	This improved us by .8%!

	new kaggle score: 85.642% :)

	Refactored fallback smoothing with supplement into external functions.
	Re-ran naive approach to make sure it is still consistent - no differences in files (trigram-supplement-naive) vs 
	...first-word.csv, so will not be submitting that new file

	Produced trigram-supplement-complex.csv to test on kaggle
	This now takes all words from the supplemental file and organizes them in dict[word][entity] = count(word, entity)
	Now our complex function will return that count or 0 if there is no match for the pair.  if the word isn't in 
	that supplemental dict at all, it falls back to trivial add-one smoothing.

	went up .93%, so pretty much 1%.

	new kaggle score: 86.581%!